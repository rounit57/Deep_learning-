{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path, sep=',', header=None, names=[\"en\", \"hi\"], skip_blank_lines=True, index_col=None)\n",
    "    data = data[data['hi'].notna()]\n",
    "    data = data[data['en'].notna()]\n",
    "    data = data[['en', 'hi']]\n",
    "    return data\n",
    "\n",
    "train = load_data(\"/home/tenet/Documents/Deep_L_Ass/Assignment_03/aksharantar_sampled/hin/hin_train.csv\")\n",
    "dev = load_data(\"/home/tenet/Documents/Deep_L_Ass/Assignment_03/aksharantar_sampled/hin/hin_valid.csv\")\n",
    "test = load_data(\"/home/tenet/Documents/Deep_L_Ass/Assignment_03/aksharantar_sampled/hin/hin_test.csv\")\n",
    "\n",
    "x_train = train['en'].values\n",
    "y_train = train['hi'].values\n",
    "\n",
    "x_val = dev['en'].values\n",
    "y_val = dev['hi'].values\n",
    "\n",
    "\n",
    "english_tokens = set()\n",
    "hindi_tokens = set()\n",
    "\n",
    "for xx, yy in zip(x, y):\n",
    "    for ch in xx:\n",
    "        english_tokens.add(ch)\n",
    "    for ch in yy:\n",
    "        hindi_tokens.add(ch)\n",
    "\n",
    "english_tokens = sorted(list(english_tokens))\n",
    "hindi_tokens = sorted(list(hindi_tokens))\n",
    "\n",
    "eng_token_map = dict([(ch, i + 1) for i, ch in enumerate(english_tokens)])\n",
    "hin_token_map = dict([(ch, i + 1) for i, ch in enumerate(hindi_tokens)])\n",
    "\n",
    "hin_token_map[\" \"] = 0\n",
    "eng_token_map[\" \"] = 0\n",
    "\n",
    "max_eng_len = max([len(i) for i in x])\n",
    "max_hin_len = max([len(i) for i in y])\n",
    "\n",
    "def process(data):\n",
    "    x, y = data['en'].values, data['hi'].values\n",
    "    # y = \"\\t\" + y + \"\\n\"\n",
    "    \n",
    "    a = np.zeros((len(x), max_eng_len), dtype=\"float32\")\n",
    "    b = np.zeros((len(y), max_hin_len), dtype=\"float32\")\n",
    "    c = np.zeros((len(y), max_hin_len, len(hindi_tokens) + 1), dtype=\"int\")\n",
    "    \n",
    "    for i, (xx, yy) in enumerate(zip(x, y)):\n",
    "        for j, ch in enumerate(xx):\n",
    "            a[i, j] = eng_token_map[ch]\n",
    "\n",
    "        a[i, j + 1:] = eng_token_map[\" \"]\n",
    "        for j, ch in enumerate(yy):\n",
    "            b[i, j] = hin_token_map[ch]\n",
    "\n",
    "            if j > 0:\n",
    "                c[i, j - 1, hin_token_map[ch]] = 1\n",
    "\n",
    "        b[i, j + 1:] = hin_token_map[\" \"]\n",
    "        c[i, j:, hin_token_map[\" \"]] = 1\n",
    "        \n",
    "    return a, b, c\n",
    "\n",
    "trainx, trainxx, trainy = process(train)\n",
    "valx, valxx, valy = process(dev)\n",
    "# testx, testxx, testy = process(test)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "reverse_eng_map = dict([(i, char) for char, i in eng_token_map.items()])\n",
    "reverse_hin_map = dict([(i, char) for char, i in hin_token_map.items()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def build_model(cell=\"LSTM\", nunits=32, enc_layers=1, dec_layers=1, embed_dim=32, dense_size=32, dropout=None):\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.embedding = nn.Embedding(num_embeddings=len(english_tokens) + 1, embedding_dim=embed_dim)\n",
    "            \n",
    "            if cell == \"LSTM\":\n",
    "                self.rnn_layers = nn.ModuleList([nn.LSTM(embed_dim, nunits, batch_first=True, return_sequences=True)] * (enc_layers - 1))\n",
    "                self.rnn_fin = nn.LSTM(embed_dim, nunits, batch_first=True, return_state=True)\n",
    "            elif cell == \"Simple\":\n",
    "                self.rnn_layers = nn.ModuleList([nn.RNN(embed_dim, nunits, batch_first=True, return_sequences=True)] * (enc_layers - 1))\n",
    "                self.rnn_fin = nn.RNN(embed_dim, nunits, batch_first=True, return_state=True)\n",
    "            elif cell == \"GRU\":\n",
    "                self.rnn_layers = nn.ModuleList([nn.GRU(embed_dim, nunits, batch_first=True, return_sequences=True)] * (enc_layers - 1))\n",
    "                self.rnn_fin = nn.GRU(embed_dim, nunits, batch_first=True, return_state=True)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            for rnn in self.rnn_layers:\n",
    "                if dropout is not None:\n",
    "                    x = nn.Dropout(dropout)(x)\n",
    "                x, _ = rnn(x)\n",
    "                \n",
    "            _, (state_h, state_c) = self.rnn_fin(x)\n",
    "            encoder_states = (state_h, state_c)\n",
    "            return encoder_states\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.embedding = nn.Embedding(num_embeddings=len(hindi_tokens) + 1, embedding_dim=embed_dim)\n",
    "            \n",
    "            if cell == \"LSTM\":\n",
    "                self.rnn_layers = nn.ModuleList([nn.LSTM(embed_dim, nunits, batch_first=True, return_sequences=True, return_state=True)] * dec_layers)\n",
    "            elif cell == \"Simple\":\n",
    "                self.rnn_layers = nn.ModuleList([nn.RNN(embed_dim, nunits, batch_first=True, return_sequences=True, return_state=True)] * dec_layers)\n",
    "            elif cell == \"GRU\":\n",
    "                self.rnn_layers = nn.ModuleList([nn.GRU(embed_dim, nunits, batch_first=True, return_sequences=True, return_state=True)] * dec_layers)\n",
    "        \n",
    "        def forward(self, x, encoder_states):\n",
    "            x = self.embedding(x)\n",
    "            for rnn in self.rnn_layers:\n",
    "                x, _ = rnn(x, encoder_states)\n",
    "            \n",
    "            return x\n",
    "\n",
    "    class Translator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Translator, self).__init__()\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.dense1 = nn.Linear(nunits, dense_size)\n",
    "            self.dense2 = nn.Linear(dense_size, len(hindi_tokens) + 1)\n",
    "            \n",
    "        def forward(self, encoder_inputs, decoder_inputs):\n",
    "            encoder_states = self.encoder(encoder_inputs)\n",
    "            decoder_output = self.decoder(decoder_inputs, encoder_states)\n",
    "            pre_output = self.dense1(decoder_output)\n",
    "            final_output = self.dense2(pre_output)\n",
    "            return final_output\n",
    "\n",
    "    model = Translator()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def accuracy1(real, pred):\n",
    "    real = torch.argmax(real, dim=2)\n",
    "    pred = torch.argmax(pred, dim=2)\n",
    "    mask = torch.logical_not(torch.eq(real, 0))\n",
    "    acc = torch.eq(real, pred)\n",
    "    mask = mask.to(torch.int32)\n",
    "    acc = acc.to(torch.int32)\n",
    "    acc = torch.mul(acc, mask)\n",
    "    mask = torch.sum(mask, dim=1)\n",
    "    acc = torch.sum(acc, dim=1)\n",
    "    acc = torch.eq(acc, mask)\n",
    "    acc = acc.to(torch.float32)\n",
    "    return torch.mean(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, nunits, dense_size, enc_layers, dec_layers, cell, dropout, embed_dim):\n",
    "        super(Model, self).__init__()\n",
    "        # Define the layers and architecture of your model here\n",
    "        # Example:\n",
    "        self.encoder = nn.LSTM(input_size=embed_dim, hidden_size=nunits, num_layers=enc_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nunits, dense_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of your model here\n",
    "        # Example:\n",
    "        x, _ = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "train = Model(nunits=256,\n",
    "              dense_size=512,\n",
    "              enc_layers=3,\n",
    "              dec_layers=1,\n",
    "              cell=\"LSTM\",\n",
    "              dropout=0.2,\n",
    "              embed_dim=256)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(train.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the accuracy function (equivalent to accuracy1)\n",
    "def accuracy1(real, pred):\n",
    "    pred = pred.argmax(dim=2)\n",
    "    mask = real != 0\n",
    "    acc = (real == pred) & mask\n",
    "    mask = mask.to(torch.int32)\n",
    "    acc = acc.to(torch.int32)\n",
    "    acc = acc * mask\n",
    "    mask = mask.sum(dim=1)\n",
    "    acc = acc.sum(dim=1)\n",
    "    acc = acc == mask\n",
    "    acc = acc.to(torch.float32)\n",
    "    return acc.mean()\n",
    "\n",
    "# Compile the model (no equivalent in PyTorch)\n",
    "# Instead, we manually call the optimizer and loss function during training\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = train(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = accuracy1(targets, outputs)\n",
    "\n",
    "    # Print training information\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, num_epochs, loss.item(), acc.item()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a variable to keep track of the best validation accuracy\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# ...\n",
    "\n",
    "# Inside your training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Perform the training steps\n",
    "    # ...\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    # Compute the validation accuracy using your accuracy function (accuracy1)\n",
    "    val_accuracy = accuracy1(val_targets, val_outputs)\n",
    "\n",
    "    # Check if the current validation accuracy is better than the best so far\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        # Update the best validation accuracy\n",
    "        best_val_accuracy = val_accuracy\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        torch.save(train.state_dict(), 'best_model.pth')\n",
    "        print('Saved the best model!')\n",
    "\n",
    "    # ...\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fit([trainx,trainxx],trainy,\n",
    "             batch_size=64,\n",
    "             validation_data = ([valx,valxx],valy),\n",
    "             epochs=20,\n",
    "             callbacks = [model_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def inference_models(model, nunits=32, enc_layers=1, dec_layers=1, cell='LSTM', dropout=None):\n",
    "    encoder_inputs = model.input[0]\n",
    "    encoder_embedding = model.get_layer('enc_embed')\n",
    "    encoder_context = encoder_embedding(encoder_inputs)\n",
    "    decoder_inputs = model.input[1]\n",
    "    decoder_embedding = model.get_layer('dec_embed')\n",
    "    decoder_context = decoder_embedding(decoder_inputs)\n",
    "\n",
    "    encoder_prev = [model.get_layer(f'enc_{i}') for i in range(enc_layers-1)]\n",
    "    encoder_fin = model.get_layer(f'enc_{enc_layers-1}')\n",
    "    temp = encoder_context\n",
    "    for i, lay in enumerate(encoder_prev):\n",
    "        temp = lay(temp)\n",
    "        if dropout is not None:\n",
    "            temp = model.get_layer(f'do_{i}')(temp)\n",
    "\n",
    "    if cell == \"LSTM\":\n",
    "        _, (state_h, state_c) = encoder_fin(temp)\n",
    "        encoder_states = [(state_h, state_c)]\n",
    "\n",
    "    elif cell == \"GRU\":\n",
    "        _, state = encoder_fin(temp)\n",
    "        encoder_states = [state]\n",
    "\n",
    "    encoder_model = nn.Sequential(*[encoder_inputs], encoder_states)\n",
    "\n",
    "    decoder = [model.get_layer(f'dec_{i}') for i in range(dec_layers)]\n",
    "\n",
    "    if cell == \"LSTM\":\n",
    "        state_inputs = []\n",
    "        state_outputs = []\n",
    "\n",
    "        decoder_input_h = nn.Input(shape=(nunits,), name='inputh0')\n",
    "        decoder_input_c = nn.Input(shape=(nunits,), name='inputc0')\n",
    "        temp, (sh, sc) = decoder[0](decoder_context, initial_state=[decoder_input_h, decoder_input_c])\n",
    "        state_inputs += [decoder_input_h, decoder_input_c]\n",
    "        state_outputs += [sh, sc]\n",
    "\n",
    "        for i in range(1, dec_layers):\n",
    "            decoder_input_h = nn.Input(shape=(nunits,), name=f'inputh{i}')\n",
    "            decoder_input_c = nn.Input(shape=(nunits,), name=f'inputc{i}')\n",
    "            temp, (sh, sc) = decoder[i](temp, initial_state=[decoder_input_h, decoder_input_c])\n",
    "            state_inputs += [decoder_input_h, decoder_input_c]\n",
    "            state_outputs += [sh, sc]\n",
    "\n",
    "        decoder_input_pass = [decoder_inputs] + state_inputs\n",
    "\n",
    "    elif cell == \"GRU\":\n",
    "        state_inputs = []\n",
    "        state_outputs = []\n",
    "\n",
    "        state_input = nn.Input(shape=(nunits,), name='inputs0')\n",
    "        temp, s = decoder[0](decoder_context, initial_state=state_input)\n",
    "        state_inputs.append(state_input)\n",
    "        state_outputs.append(s)\n",
    "\n",
    "        for i in range(1, dec_layers):\n",
    "            state_input = nn.Input(shape=(nunits,), name=f'inputs{i}')\n",
    "            temp, s = decoder[i](temp, initial_state=state_input)\n",
    "            state_inputs.append(state_input)\n",
    "            state_outputs.append(s)\n",
    "\n",
    "        decoder_input_pass = [decoder_inputs] + state_inputs\n",
    "\n",
    "    pre_out = model.get_layer('dense1')(temp)\n",
    "    final_output = model.get_layer('dense2')(pre_out)\n",
    "\n",
    "    decoder_model = nn.Sequential(*decoder_input_pass, final_output, *state_outputs)\n",
    "\n",
    "    return encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc,dec = inference_models(model,nunits=256,enc_layers=3,dec_layers=1,cell=\"LSTM\",dropout='yes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
